{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6716564,"sourceType":"datasetVersion","datasetId":3869963},{"sourceId":6780480,"sourceType":"datasetVersion","datasetId":3901735},{"sourceId":7110397,"sourceType":"datasetVersion","datasetId":4099724},{"sourceId":7555627,"sourceType":"datasetVersion","datasetId":4400407}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nif False:\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n\n            import os\nimport re\nimport glob\nimport pathlib\nimport time\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport cv2\n\nimport PIL\nfrom PIL import Image\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\n\nfrom collections import Counter\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nSEED=42 # Also use 442, 4442, 422, 4222\nnp.random.seed(SEED)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.optimizers import Adam, SGD, RMSprop\nfrom tensorflow.keras.layers import Dropout, BatchNormalization\nfrom tensorflow.keras.layers import (\n    Input, Dense, Conv2D, Flatten, Activation, \n    MaxPooling2D, AveragePooling2D, ZeroPadding2D, GlobalAveragePooling2D, GlobalMaxPooling2D, add\n)\n\nfrom tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom tensorflow.keras.utils import plot_model\n\nfrom tensorflow.keras.applications.vgg19 import VGG19\nfrom tensorflow.keras.applications.vgg19 import preprocess_input\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom tensorflow.keras.applications.inception_resnet_v2 import preprocess_input","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG = dict(\n        batch_size        =  16,     # 8; 16; 32; 64; bigger batch size => memory allocation issue\n        epochs            =  20,   # 5; 10; 20;\n        verbose           =   1,    # 0; 1\n        workers           =   4,    # 1; 2; 3\n\n        optimizer         = 'adam', # 'SGD', 'RMSprop'\n\n        RANDOM_STATE      =  123,   \n    \n        # Path to save a model\n        path_model        = '../working/',\n\n        # Images sizes - 224 for ResNet conversion (default is 32x32)\n        img_size          = 224, \n        img_height        = 224, \n        img_width         = 224, \n\n        # Images augs\n        ROTATION          = 180.0,\n        ZOOM              =  10.0,\n        ZOOM_RANGE        =  [0.9,1.1],\n        HZOOM             =  10.0,\n        WZOOM             =  10.0,\n        HSHIFT            =  10.0,\n        WSHIFT            =  10.0,\n        SHEAR             =   5.0,\n        HFLIP             = True,\n        VFLIP             = True,\n\n        # Postprocessing\n        label_smooth_fac  =  0.00,  # 0.01; 0.05; 0.1; 0.2;    \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASEPATH = \"/kaggle/input/fitzpatrickdata\"\ndf_train_full = pd.read_csv(os.path.join(BASEPATH, 'train17.csv'))\ndf_test  = pd.read_csv(os.path.join(BASEPATH, 'train177.csv'))\ndf_sub   = pd.read_csv(os.path.join(BASEPATH, 'sample_submission177.csv'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path = '/kaggle/input/fitzpatrick2class/Fitzpatrick2Class/Train'\ntest_path = '/kaggle/input/fitzpatrick2class/Fitzpatrick2Class/Test'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir = pathlib.Path(train_path)\ntest_dir = pathlib.Path(test_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Define the data directory\ndata_directory = '/kaggle/input/fitzpatrick2class/Fitzpatrick2Class' #NEEDS TO BE SORTED BY LABEL/TrainTest\n\n# Define the ImageDataGenerator \ndatagen = ImageDataGenerator(\n    rescale=1.0/255.0,      # Rescale pixel values to the range [0, 1]\n    rotation_range=20,      # Randomly rotate images by up to 20 degrees\n    width_shift_range=0.2,  # Randomly shift images horizontally by up to 20% of the width\n    height_shift_range=0.2, # Randomly shift images vertically by up to 20% of the height\n    horizontal_flip=True,   # Randomly flip images horizontally\n    validation_split=0.2    # Split the data into training (80%) and validation (20%)\n)\n\n# Create the training data generator\ntrain_generator = datagen.flow_from_directory(\n    data_directory,\n    target_size=(224, 224), # Resize images to 224x224 pixels (adjust as needed)\n    batch_size=32,          # Batch size for training\n    class_mode='binary',    # Set class_mode to 'binary' or 'categorical' based on your problem\n    subset='training'       # Specify that this is the training set\n)\n\n# Create the validation data generator\nvalidation_generator = datagen.flow_from_directory(\n    data_directory,\n    target_size=(224, 224), # Resize images to 224x224 pixels (should match training)\n    batch_size=32,          # Batch size for validation\n    class_mode='binary',    \n    subset='validation'     \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count number of images in each set.\nimg_count_train = len(list(train_dir.glob('*/*.jpg')))\nimg_count_test  = len(list(test_dir.glob('*/*.jpg')))\nprint('{} train images'.format(img_count_train))\nprint('{} test  images'.format(img_count_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import InceptionResNetV2\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\n\ndata_directory = '/kaggle/input/fitzpatrick2class/Fitzpatrick2Class' #NEEDS TO BE SORTED BY LABEL\n\ndatagen = ImageDataGenerator(\n    rescale=1.0/255.0,          \n    rotation_range=20,          \n    width_shift_range=0.2,      \n    height_shift_range=0.2,     \n    horizontal_flip=True,       \n    zoom_range=0.2,             \n    brightness_range=[0.8, 1.2] \n)\n\ndata_generator = datagen.flow_from_directory(\n    data_directory,\n    target_size=(224, 224),    \n    batch_size=32,\n    class_mode='binary',      \n    shuffle=True              \n)\n \nbase_model = VGG19(weights='imagenet', include_top=False)\n\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(1024, activation='relu')(x)\npredictions = Dense(1, activation='sigmoid')(x)\n\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nmodel.fit(data_generator, epochs=20, steps_per_epoch=len(data_generator))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = model.evaluate(validation_generator)[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Accuracy: {accuracy * 100:.2f}%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_file_path = '/kaggle/working/.h5'\nmodel.save(model_file_path)\nprint(f\"Model saved to {model_file_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.models import load_model\n\nmodel = load_model('/kaggle/working/.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\n\n# Define the directory containing test images\ntest_image_dir = '/kaggle/input/fitzpatrick2class/Fitzpatrick2Class'\n\n# List to store predictions and corresponding image filenames\npredictions = []\nimage_filenames = []\n\n# Iterate through test images\nfor image_filename in os.listdir(test_image_dir):\n    if image_filename.endswith('.jpg'):\n        image_path = os.path.join(test_image_dir, image_filename)\n        # Load and preprocess the image\n        img = cv2.imread(image_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n        img = cv2.resize(img, (224, 224))  # Resize to match model input size\n        img = img / 255.0  # Normalize pixel values to [0, 1]\n        img = np.expand_dims(img, axis=0)  # Add batch dimension\n        \n        # Make a prediction\n        prediction = model.predict(img)\n        predictions.append(prediction[0][0])  \n        image_filenames.append(image_filename)  \n\nif len(predictions) == len(image_filenames):\n    # Create a DataFrame for predictions\n    df_predictions = pd.DataFrame({\n        'image_name': image_filenames,\n        'target': predictions\n    })\n\n    # Save the predictions to a CSV file\n    df_predictions.to_csv('submission.csv', index=False)\nelse:\n    print(\"Error: Predictions and image filenames have different lengths.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow\n!pip install tensorflow-hub\n!pip install tf-slim\n!pip install opencv-python","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow==2.5\n!pip install opencv-python-headless\n\n!git clone https://github.com/tensorflow/models.git\n\nimport os\nos.chdir('/kaggle/working/models/research')\n!protoc object_detection/protos/*.proto --python_out=.\n\nwith open('object_detection/packages/tf2/setup.py', 'a') as f:\n    f.write(\"\\\"tf-models-official\\\",\\n\")\n\n!python -m pip install .\n!python object_detection/builders/model_builder_tf2_test.py\n!python /kaggle/working/models/research/object_detection/model_main_tf2.py \\\n   --pipeline_config_path=PATH_TO_CONFIG_FILE \\\n   --model_dir=MODEL_DIR \\\n   --num_train_steps=NUM_TRAIN_STEPS \\\n   --sample_1_of_n_eval_examples=1 \\\n   --alsologtostderr\n!python /kaggle/working/models/research/object_detection/exporter_main_v2.py \\\n   --input_type=image_tensor \\\n   --pipeline_config_path=PATH_TO_CONFIG_FILE \\\n   --trained_checkpoint_dir=MODEL_DIR \\\n   --output_directory=EXPORT_DIR","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = tf.keras.applications.VGG19(\n    weights='imagenet', \n    include_top=False, \n    input_tensor=input_layer\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = GlobalAveragePooling2D()(base_model.output)\nx = Dense(1024, activation='relu')(x)\noutput_layer = Dense(1, activation='sigmoid')(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(inputs=input_layer, outputs=output_layer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = images / 255.0\nsplit_ratio = 0.8\nsplit_index = int(len(images) * split_ratio)\n\ntrain_images, test_images = images[:split_index], images[split_index:]\ntrain_labels, test_labels = labels[:split_index], labels[split_index:]\n\n# Define the model\ninput_shape = (*images.shape[1:],)\ninput_layer = Input(shape=input_shape)\nx = Conv2D(32, (3, 3), activation='relu')(input_layer)\nx = MaxPooling2D((2, 2))(x)\nx = Conv2D(64, (3, 3), activation='relu')(x)\nx = MaxPooling2D((2, 2))(x)\nx = Flatten()(x)\nx = Dense(128, activation='relu')(x)\noutput_layer = Dense(4, activation='linear')(x)\n\nmodel = Model(inputs=input_layer, outputs=output_layer)\nmodel.compile(optimizer='adam', loss='mse')  \n\n# Train the model\nhistory = model.fit(train_images, train_labels, epochs=20, batch_size=32, validation_split=0.2)\n\n# Evaluate the model on the test set and calculate accuracy\nloss = model.evaluate(test_images, test_labels)\naccuracy = 100.0 - loss  # You can consider this as a simple accuracy metric\n\nprint(f'Test Loss: {loss}')\nprint(f'Test Accuracy: {accuracy:.2f}%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}